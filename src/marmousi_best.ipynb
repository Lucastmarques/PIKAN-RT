{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTcdlLdHKxlS"
   },
   "source": [
    "# **Applying KAN-PIN to the Marmousi Velocity Model**  \n",
    "\n",
    "For the Marmousi velocity model, where velocity variations are highly complex, KANs might overfit to noise or spurious correlations. \n",
    "By combining Physics Informed Neural Network, we can mitigate this by ensuring that the learned representations align with expected physical behaviors in ray tracing, improving generalization and stability. \n",
    "This approach allows KANs to learn meaningful solutions that are both data-efficient and physically valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells configure the environment for the experiment. This includes:\n",
    "*   **Loading extensions and libraries**: Essential for data manipulation, deep learning, and visualization.\n",
    "*   **Device configuration**: Sets the computation device to CUDA if available, otherwise defaults to CPU.\n",
    "*   **Path definitions**: Specifies the paths for data, output, and model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11708,
     "status": "ok",
     "timestamp": 1730982500394,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "8YiCb8apKxlU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m2ai/miniconda3/envs/exai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "from itertools import cycle, product\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from kan import KAN\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Sampler\n",
    "from utils.architecture import Architecture\n",
    "from rt_python import DataGeneratorMarmousi\n",
    "from utils.metrics import score\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CUDA device when available. Otherwise, use CPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730982500394,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "4z6Ns_hKKxlV",
    "outputId": "a2f6be86-a014-4254-f62a-c6c183a19a97"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")  # Force CPU for this example\n",
    "print(f\"Using '{DEVICE}' device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup your path to data, output and checkpoint folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/best\")\n",
    "OUTPUT_PATH = Path(\"../output/best\")\n",
    "CHECKPOINT_PATH = OUTPUT_PATH/\"model\"\n",
    "\n",
    "model_save_path = \"../output/best/best.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-37zvmBqKxlW"
   },
   "source": [
    "## **Data Acquisition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this study is derived from the Marmousi velocity model, a well-known synthetic benchmark in geophysics. This model represents a highly complex subsurface with strong velocity variations, making it ideal for testing ray-tracing algorithms and machine-learning approaches in seismic imaging.\n",
    "\n",
    "To improve numerical stability and facilitate more efficient training, we normalize the velocity values by converting them to **km/s**. This transformation mitigates scale discrepancies (e.g., values ranging from 10 to 10,000), ensuring better gradient propagation and more stable optimization during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "executionInfo": {
     "elapsed": 94916,
     "status": "ok",
     "timestamp": 1730982595308,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "F3kXuCCaJCGR",
    "outputId": "10afa0c9-4cd1-4fd6-fca6-da0935054911"
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "nx = 2301  # Number of samples in the distance dimension\n",
    "nz = 751   # Number of samples in the depth dimension\n",
    "dz = 4     # Distance increment (m)\n",
    "dx = 4     # Depth increment (m)\n",
    "\n",
    "# Model limits in km\n",
    "xmax = nx * dx / 1000\n",
    "zmax = nz * dz / 1000\n",
    "\n",
    "vp_file = \"../data/marmousi_vp.bin\"\n",
    "\n",
    "vp = np.fromfile(vp_file, dtype=np.dtype('float32').newbyteorder('<'))\n",
    "vp = vp.reshape((nx, nz)).transpose() / 1000  # Converting to km/s\n",
    "vp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Marmousi Velocity Model with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.imshow(vp, extent=[0, xmax, zmax, 0], aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label=\"Velocity (km/s)\")\n",
    "plt.title(\"Marmousi - Vp\")\n",
    "plt.xlabel(\"x (km)\")\n",
    "plt.ylabel(\"z (km)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ray-Tracing Data Generation and Visualization Using `DataGeneratorMarmousi`**\n",
    "\n",
    "The **`DataGeneratorMarmousi`** class systematically generates ray-tracing data within the **Marmousi velocity model**, enabling machine learning-driven approaches in seismic modeling. By leveraging B-spline interpolation and numerical integration, it constructs high-quality training datasets that capture wave propagation dynamics. The class extends `DataGenerator`, inheriting core spatial attributes without redefining the constructor. Its key method, `run_multiple`, generates ray trajectories across a predefined range of initial positions `(x0, z0)` and angles `(θ0)`, ensuring comprehensive sampling of the velocity model. This process involves interpolating the velocity field for smooth representation, numerically solving ray equations, and aggregating the computed paths into a structured **DataFrame**. The resulting dataset is optimized for training **Kolmogorov-Arnold Networks (KANs)** and other ML models, providing a physics-informed foundation for learning seismic wave behavior while maintaining computational efficiency.  \n",
    "\n",
    "The code below initializes an instance of `DataGeneratorMarmousi` and generates a dataset of ray-tracing paths within the **Marmousi velocity model**. The process consists of three main steps:  \n",
    "\n",
    "1. **Defining the Spatial Domain**:  \n",
    "   The `x_range` and `z_range` parameters define the horizontal and depth extents of the velocity model, ranging from `0` to `xmax` and `0` to `zmax`, respectively. These boundaries ensure that rays are traced within the predefined seismic model.  \n",
    "\n",
    "2. **Generating Ray-Tracing Data (`run_multiple`)**:  \n",
    "   The method `run_multiple` is called to compute multiple ray trajectories, systematically sampling different initial conditions:\n",
    "   - **`x0_range=(4, 6)` and `z0_range=(1, 2)`**: Specifies that rays originate from positions within these spatial bounds.  \n",
    "   - **`theta_range=(45, 75)`**: Defines the range of initial propagation angles in degrees.  \n",
    "   - **`vp=vp`**: Passes the Marmousi velocity model as input.  \n",
    "   - **`factor=30`**: Controls the downsampling of the velocity field for computational efficiency.  \n",
    "   - **`dx_dy=0.1`**: Sets the resolution for sampling initial positions.  \n",
    "\n",
    "   The method iterates over all combinations of `(x0, z0, θ0)`, computes the corresponding ray trajectories, and stores the results in a structured **DataFrame** (`df`). This dataset includes the spatial evolution of each ray over time, along with its velocity and direction of propagation.  \n",
    "\n",
    "3. **Visualization (`plot`)**:  \n",
    "   The `plot` method visualizes the generated ray paths overlaid on the velocity model. The **velocity field** is represented as a colormap, while the **ray trajectories** illustrate how seismic waves travel through the subsurface. The figure size `(22,6)` ensures clarity, and `plt.show()` displays the plot.  \n",
    "\n",
    "This workflow effectively generates a diverse dataset for **machine learning applications**, such as training **Kolmogorov-Arnold Networks (KANs)** to learn seismic wave propagation patterns while maintaining physical consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = (0, xmax)\n",
    "z_range = (0, zmax)\n",
    "\n",
    "data_gen = DataGeneratorMarmousi(\n",
    "    x_range=x_range,\n",
    "    z_range=z_range\n",
    ")\n",
    "df = data_gen.run_multiple(x0_range=(4, 6),\n",
    "                           z0_range=(1, 2),\n",
    "                           theta_range=(45, 75),\n",
    "                           vp=vp,\n",
    "                           factor=30,\n",
    "                           dx_dy=0.1,\n",
    "                           dtheta=5,\n",
    "                           t_max=0.4,\n",
    "                           )\n",
    "fig = data_gen.plot(df, figsize=(22, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the feature groups for our architecture: **KAN features** (inputs to the Kolmogorov-Arnold Network), **Arch features** (KAN features plus additional features used in the PIN module), and **target features** (the predicted output). This organization ensures a clear and structured data flow within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730982595308,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "kUd1qjKPJCGR"
   },
   "outputs": [],
   "source": [
    "kan_features = ['x0', 'z0', 'theta0_p', 't']\n",
    "arch_features = kan_features + ['pi_weight', 'dxdt', 'dzdt', 'dpxdt', 'dpzdt']\n",
    "target = ['x', 'z', 'px', 'pz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Weights in PINN Loss Function**  \n",
    "\n",
    "In **Physics-Informed Neural Networks (PINNs)**, loss weighting plays a crucial role in balancing data-driven learning with physical constraints. The weights used in the PINN loss function dynamically adjust the contribution of data and physics-based constraints. In regions with dense data points, the model prioritizes data fidelity, while in sparsely sampled areas, it emphasizes adherence to the governing equations. This adaptive weighting ensures a balanced learning process, improving generalization across the entire domain.  \n",
    "\n",
    "Here, the **weighting strategy is based on spatial frequency**. The code computes the number of data points within predefined spatial regions (squares), assigning higher weights to underrepresented areas and lower weights to densely populated regions. This **adaptive weighting** prevents the model from overfitting high-density regions and helps it generalize across the entire domain.  \n",
    "\n",
    "By incorporating these weights into the PINN loss function, the model effectively learns from both observed data and physics-based constraints, leading to improved stability, accuracy, and robustness in solving inverse problems in seismic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squares_limits(data: pd.DataFrame, restrictions: dict, step: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a set of square limits based on specified feature restrictions and step size.\n",
    "\n",
    "    This function partitions the feature space into discrete intervals based on given restrictions,\n",
    "    creating a grid of square regions for further analysis.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset containing feature columns.\n",
    "        restrictions (dict): A dictionary defining the range of each feature. \n",
    "            Each key corresponds to a feature name, and its value is a dictionary \n",
    "            with 'min' and 'max' keys specifying the range.\n",
    "        step (float): The step size used to discretize the feature space.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing all possible square intervals formed by the \n",
    "        specified feature restrictions.\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: If any feature in `restrictions` is not present in the dataset.\n",
    "\n",
    "    Example:\n",
    "        >>> data = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n",
    "        >>> restrictions = {'x': {'min': 0, 'max': 3}, 'y': {'min': 4, 'max': 6}}\n",
    "        >>> get_squares_limits(data, restrictions, step=1)\n",
    "        array([[[0., 1.], [4., 5.]],\n",
    "               [[0., 1.], [5., 6.]],\n",
    "               [[1., 2.], [4., 5.]],\n",
    "               [[1., 2.], [5., 6.]],\n",
    "               [[2., 3.], [4., 5.]],\n",
    "               [[2., 3.], [5., 6.]]], dtype=float32)\n",
    "    \"\"\"\n",
    "    assert all([feature in data.columns for feature in restrictions.keys()]), \\\n",
    "        \"Some features presented in restrictions are not in the data.\"\n",
    "\n",
    "    # Generate the intervals for each feature\n",
    "    limits_map = {}\n",
    "    for name, boundary in restrictions.items():\n",
    "        aux = np.arange(boundary['min'], boundary['max'] + step, step=step, dtype='float32')\n",
    "        aux = [round(x, 3) for x in aux]\n",
    "        limits_map[name] = [(aux[i], aux[i+1]) for i in range(len(aux) - 1)]\n",
    "\n",
    "    # Create all combinations of the intervals between features\n",
    "    combinations = list(product(*limits_map.values()))\n",
    "\n",
    "    result = [np.array(combination, dtype='float32') for combination in combinations]\n",
    "\n",
    "    return np.array(result, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(data: pd.DataFrame, restrictions: dict, step: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes the frequency of data points within predefined square regions in the feature space.\n",
    "\n",
    "    This function divides the input data into grid-based square regions and counts the number \n",
    "    of data points falling within each region. The result is stored in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset containing the features to be analyzed.\n",
    "        restrictions (dict): A dictionary defining feature-wise range restrictions.\n",
    "            Each key corresponds to a feature name, with 'min' and 'max' specifying the range.\n",
    "        step (float, optional): The step size used to define square regions. Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing each square's limits and the corresponding data point count.\n",
    "\n",
    "    Example:\n",
    "        >>> data = pd.DataFrame({'x': [0.5, 1.5, 2.5], 'y': [4.5, 5.5, 6.5]})\n",
    "        >>> restrictions = {'x': {'min': 0, 'max': 3}, 'y': {'min': 4, 'max': 6}}\n",
    "        >>> get_frequency(data, restrictions, step=1)\n",
    "           square   frequency\n",
    "        0  [[0, 1], [4, 5]]  1\n",
    "        1  [[0, 1], [5, 6]]  0\n",
    "        2  [[1, 2], [4, 5]]  0\n",
    "        3  [[1, 2], [5, 6]]  1\n",
    "        4  [[2, 3], [4, 5]]  0\n",
    "        5  [[2, 3], [5, 6]]  1\n",
    "    \"\"\"\n",
    "    squares_limits = get_squares_limits(data, restrictions, step)\n",
    "\n",
    "    # Count points in each square\n",
    "    frequencies = []\n",
    "    for square in squares_limits:\n",
    "        mask = np.ones(len(data), dtype=bool)\n",
    "        for feature, limits in zip(restrictions.keys(), square):\n",
    "            sqr_min, sqr_max = limits\n",
    "            mask &= (data[feature] >= sqr_min) & (data[feature] < sqr_max)\n",
    "\n",
    "        # Count points inside this square\n",
    "        frequencies.append(np.sum(mask))\n",
    "\n",
    "    # Prepare result as a DataFrame\n",
    "    frequency_df = pd.DataFrame(\n",
    "        data={\n",
    "            \"square\": list(squares_limits),\n",
    "            \"frequency\": frequencies\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return frequency_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frequency_to_data(data: pd.DataFrame, frequency_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds frequency information to the dataset based on predefined spatial regions.\n",
    "\n",
    "    This function assigns a frequency value to each data point by matching it to the \n",
    "    corresponding region (square) defined in `frequency_df`. The frequency represents \n",
    "    the number of data points found in that region, ensuring that each sample is \n",
    "    weighted accordingly for further processing.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset containing feature columns.\n",
    "        frequency_df (pd.DataFrame): A DataFrame with frequency counts for different \n",
    "            spatial regions, where each row contains:\n",
    "            - \"square\": A list of tuples defining the boundaries of the region.\n",
    "            - \"frequency\": The number of data points within that region.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of `data` with an added \"frequency\" column.\n",
    "\n",
    "    Example:\n",
    "        >>> data = pd.DataFrame({'x': [0.5, 1.5, 2.5], 'y': [4.5, 5.5, 6.5]})\n",
    "        >>> frequency_df = pd.DataFrame({\n",
    "        ...     \"square\": [[(0,1), (4,5)], [(1,2), (5,6)], [(2,3), (6,7)]],\n",
    "        ...     \"frequency\": [10, 5, 3]\n",
    "        ... })\n",
    "        >>> add_frequency_to_data(data, frequency_df)\n",
    "             x    y  frequency\n",
    "        0  0.5  4.5        10\n",
    "        1  1.5  5.5         5\n",
    "        2  2.5  6.5         3\n",
    "    \"\"\"\n",
    "    # Initialize an array to store frequency values\n",
    "    frequencies = np.zeros(len(data), dtype=int)\n",
    "\n",
    "    # Iterate over each row in the frequency DataFrame to assign frequencies\n",
    "    for _, square_row in frequency_df.iterrows():\n",
    "        square = square_row['square']\n",
    "        frequency = square_row['frequency']\n",
    "\n",
    "        # Build a mask to filter the rows in `data` that fall within the current square\n",
    "        mask = np.ones(len(data), dtype=bool)\n",
    "        for feature, (min_val, max_val) in zip(data.columns, square):\n",
    "            mask &= (data[feature] >= min_val) & (data[feature] < max_val)\n",
    "\n",
    "        # Assign the frequency value to the matching rows\n",
    "        frequencies[mask] = frequency\n",
    "\n",
    "    # Create a copy of the dataset and add the frequency column\n",
    "    data_with_frequency = data.copy()\n",
    "    data_with_frequency['frequency'] = frequencies\n",
    "\n",
    "    return data_with_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(data: pd.DataFrame, filename: str = \"output/surface_plot.html\") -> None:\n",
    "    \"\"\"\n",
    "    Generates a 3D surface plot of point frequencies within spatial regions.\n",
    "\n",
    "    This function visualizes the frequency distribution of data points across \n",
    "    predefined grid regions. It extracts midpoints of the spatial squares and \n",
    "    maps the corresponding frequencies, creating a structured 3D surface plot \n",
    "    using Plotly.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing:\n",
    "            - \"square\": A list of tuples defining the spatial region boundaries.\n",
    "            - \"frequency\": The count of data points within each square.\n",
    "        filename (str, optional): The output file path to save the interactive \n",
    "            HTML plot. Default is `\"output/surface_plot.html\"`.\n",
    "\n",
    "    Returns:\n",
    "        None: The function generates and saves the plot but does not return a value.\n",
    "\n",
    "    Example:\n",
    "        >>> data = pd.DataFrame({\n",
    "        ...     \"square\": [[(0,1), (4,5)], [(1,2), (5,6)], [(2,3), (6,7)]],\n",
    "        ...     \"frequency\": [10, 5, 3]\n",
    "        ... })\n",
    "        >>> plot_surface(data, filename=\"surface_plot.html\")\n",
    "    \"\"\"\n",
    "    # Extract midpoints and corresponding frequencies\n",
    "    squares = data['square'].tolist()  # Assuming 'square' column stores lists\n",
    "    frequencies = data['frequency'].values\n",
    "\n",
    "    # Compute midpoints for visualization\n",
    "    x = np.array([(interval[0][0] + interval[0][1]) / 2 for interval in squares])\n",
    "    y = np.array([(interval[1][0] + interval[1][1]) / 2 for interval in squares])\n",
    "    z = frequencies\n",
    "\n",
    "    # Create a structured grid for the surface plot\n",
    "    unique_x = np.unique(x)\n",
    "    unique_y = np.unique(y)\n",
    "    X, Z = np.meshgrid(unique_x, unique_y)\n",
    "\n",
    "    # Map frequency values to the grid\n",
    "    freq = np.zeros_like(X)\n",
    "    for i, x_val in enumerate(unique_x):\n",
    "        for j, y_val in enumerate(unique_y):\n",
    "            mask = (x == x_val) & (y == y_val)\n",
    "            if np.any(mask):\n",
    "                freq[j, i] = z[mask][0]  # Assign the frequency to the grid point\n",
    "\n",
    "    # Create 3D surface plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Surface(z=freq, x=X, y=Z, opacity=0.8))\n",
    "\n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Z',\n",
    "            zaxis_title='Frequency'\n",
    "        ),\n",
    "        title='Surface Plot of Points in Squares'\n",
    "    )\n",
    "\n",
    "    # Save plot as an interactive HTML file\n",
    "    fig.write_html(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vyP1MNzJCGR"
   },
   "source": [
    "### **Splitting the Dataset into Training, Validation, and Test Sets**  \n",
    "\n",
    "Proper dataset partitioning is essential to ensure robust model evaluation and generalization. Here, we divide the data into **training, validation, and test sets**, ensuring that the model learns effectively while being evaluated on unseen data. The **training set** is used to optimize model parameters, the **validation set** helps fine-tune hyperparameters and prevent overfitting, and the **test set** provides an unbiased estimate of final model performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730982595308,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "TpM1Iu3VJCGR",
    "outputId": "1cea34dc-0f52-4d04-de8c-ddce79aa57ef"
   },
   "outputs": [],
   "source": [
    "initial_conditions = df[['x0', 'z0', 'theta0_p']].drop_duplicates()\n",
    "aux = initial_conditions.copy()\n",
    "\n",
    "ic_train = aux.sample(frac=0.6, random_state=SEED)\n",
    "ic_val = aux.drop(ic_train.index).sample(frac=0.5, random_state=SEED)\n",
    "ic_test = aux.drop(list(ic_train.index) + list(ic_val.index))\n",
    "\n",
    "print(\"Train Initial Conditions:\", len(ic_train))\n",
    "print(\"Validation Initial Conditions:\", len(ic_val))\n",
    "print(\"Test Initial Conditions:\", len(ic_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_conditions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of **401 points** are used to construct the ray trajectory, covering a time range from **0 to 400 ms** with a time step (**dt**) of **1 ms**. This high-resolution sampling ensures precise tracking of wave propagation dynamics, improving the accuracy of trajectory estimation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730982595308,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "5ukSWLgnJCGR",
    "outputId": "d7a92f53-cc1c-4c84-d90c-978579b92ee1"
   },
   "outputs": [],
   "source": [
    "df_train = df.merge(ic_train, on=['x0', 'z0', 'theta0_p'], how='inner')\n",
    "df_val = df.merge(ic_val, on=['x0', 'z0', 'theta0_p'], how='inner')\n",
    "df_test = df.merge(ic_test, on=['x0', 'z0', 'theta0_p'], how='inner')\n",
    "\n",
    "print(\"Train Size:\", len(df_train))\n",
    "print(\"Validation Size:\", len(df_val))\n",
    "print(\"Test Size:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the generated datasets ensures future reproducibility and transparency in scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(DATA_PATH/'train_marmousi_f30.csv', index=False)\n",
    "df_val.to_csv(DATA_PATH/'val_marmousi_f30.csv', index=False)\n",
    "df_test.to_csv(DATA_PATH/'test_marmousi_f30.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the datasets, let's load them so we can skip regenerating and preprocessing later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH/'train_marmousi_f30.csv')\n",
    "df_val = pd.read_csv(DATA_PATH/'val_marmousi_f30.csv')\n",
    "df_test = pd.read_csv(DATA_PATH/'test_marmousi_f30.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are balancing learning from two sources: the actual data points (data loss) and the governing physical equations (physics loss). However, the data generated from processes like ray tracing is often not uniformly distributed across the entire domain. Some areas may be densely sampled with many ray paths crossing them, while others might be sparsely sampled.\n",
    "\n",
    "**Why is this a problem?**\n",
    "If we treat all points equally, the model's training will be dominated by the high-density regions. It will learn to be very accurate in those areas, but may neglect the sparsely populated regions, leading to poor generalization.\n",
    "\n",
    "**How do we fix it?**\n",
    "We introduce a weighting scheme for the physics loss that is inversely proportional to the data frequency.\n",
    "-   **High-Frequency Regions (many data points):** These regions receive a *lower* weight. The model can rely more on the abundant data to learn the correct behavior.\n",
    "-   **Low-Frequency Regions (few data points):** These regions receive a *higher* weight. This forces the model to pay closer attention to the governing physics (the ray-tracing equations) to make accurate predictions, compensating for the lack of data.\n",
    "\n",
    "**The Role of the Plot**\n",
    "By plotting the frequency as a 3D surface, we get an immediate visual understanding of our data distribution. This plot helps us:\n",
    "1.  **Verify Data Coverage:** Quickly identify which parts of the velocity model are well-sampled and which are not.\n",
    "2.  **Understand the Weights:** Intuitively grasp how the physics loss will be weighted across the domain. The \"valleys\" in the plot correspond to areas where the physics loss will have the most impact.\n",
    "3.  **Debug Data Generation:** Unexpected gaps or patterns in the frequency plot can signal issues in the data generation process itself.\n",
    "\n",
    "In short, plotting the frequency is a key diagnostic step that validates our strategy for creating a more robust and accurate physics-informed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictions = {\n",
    "    'x': {'min': 0, 'max': xmax},\n",
    "    'z': {'min': 0, 'max': zmax},\n",
    "}\n",
    "frequency_df = get_frequency(df_train, restrictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_surface(frequency_df, filename=OUTPUT_PATH/\"frequency_plot_marmousi_f30.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_freq = add_frequency_to_data(df_train, frequency_df)\n",
    "df_train_freq['normalized_frequency'] = (\n",
    "    (df_train_freq['frequency'] - df_train_freq['frequency'].min()) / (df_train_freq['frequency'].max() - df_train_freq['frequency'].min()))\n",
    "df_train_freq['pi_weight'] = 1 / df_train_freq['normalized_frequency']\n",
    "df_train_freq['pi_weight'] = df_train_freq['pi_weight'].clip(0, 10)\n",
    "df_train_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_freq.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_freq[arch_features+target].head().to_latex(index=False, float_format=\"{:0.2f}\".format, escape=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=df_train_freq['x'],\n",
    "    y=df_train_freq['z'],\n",
    "    z=df_train_freq['pi_weight'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=df_train_freq['pi_weight'],  # Color by pi_weight\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Add labels\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='x',\n",
    "        yaxis_title='z',\n",
    "        zaxis_title='pi_weight'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "fig.write_html(OUTPUT_PATH/\"train_pi_weight_plot_marmousi_f30.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rf48zI5JCGS"
   },
   "source": [
    "### Creating the DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Batch Sampler Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a custom `BalancedBatchSampler` to ensure that each training batch contains a consistent proportion of samples representing the initial conditions (where time `t=0`) and samples from the rest of the trajectory. This is crucial because our physics-informed loss function includes a Mean Squared Error (MSE) term that specifically penalizes deviations from these initial conditions. By guaranteeing that `t=0` points are present in every batch, we ensure that this part of the loss function is consistently applied, leading to more stable training and a model that accurately respects the initial state of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, t_index=3, shuffle=True):\n",
    "        \"\"\"\n",
    "    A sampler that generates balanced batches from a dataset, ensuring that each batch contains\n",
    "    an equal number of samples with a specific target value.\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to sample from.\n",
    "        batch_size (int): The number of samples per batch.\n",
    "        t_index (int, optional): The index of the target value in the dataset samples. Default is 4.\n",
    "        shuffle (bool, optional): Whether to shuffle the batch indices. Default is True.\n",
    "        \n",
    "    Attributes:\n",
    "        dataset (Dataset): The dataset to sample from.\n",
    "        batch_size (int): The number of samples per batch.\n",
    "        t_index (int): The index of the target value in the dataset samples.\n",
    "        shuffle (bool): Whether to shuffle the batch indices.\n",
    "        t_zero_indices (list): Indices of samples with target value equal to 0.\n",
    "        non_t_zero_indices (list): Indices of samples with target value not equal to 0.\n",
    "        \n",
    "    Methods:\n",
    "        __iter__(): Generates balanced batches of indices.\n",
    "        __len__(): Returns the number of batches.\n",
    "    \"\"\"\n",
    "        if batch_size % 2 != 0 or batch_size <= 0:\n",
    "            raise ValueError(\"batch_size must be a positive even integer.\")\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.t_index = t_index\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Split the dataset into two parts: with t=0 and without t=0\n",
    "        self.t_zero_indices = []\n",
    "        self.non_t_zero_indices = []\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            data = self.dataset[i]\n",
    "            try:\n",
    "                t_sample = data[0][self.t_index]\n",
    "            except (IndexError, TypeError):\n",
    "                raise ValueError(f\"The time index t_index={self.t_index} is not valid.\")\n",
    "            if t_sample == 0:\n",
    "                self.t_zero_indices.append(i)\n",
    "            else:\n",
    "                self.non_t_zero_indices.append(i)\n",
    "\n",
    "        if not self.t_zero_indices or not self.non_t_zero_indices:\n",
    "            raise ValueError(\"There are not enough samples in each class to create balanced batches.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.t_zero_indices)\n",
    "            random.shuffle(self.non_t_zero_indices)\n",
    "\n",
    "        t_zero_iter = cycle(self.t_zero_indices)\n",
    "        non_t_zero_iter = cycle(self.non_t_zero_indices)\n",
    "\n",
    "        num_batches = len(self.dataset) // self.batch_size\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            t_zero_batch = [next(t_zero_iter) for _ in range(2)] # Samples with t=0 per batch\n",
    "            non_t_zero_batch = [next(non_t_zero_iter) for _ in range(self.batch_size-2)]\n",
    "            batch_indices = t_zero_batch + non_t_zero_batch\n",
    "\n",
    "            if self.shuffle:\n",
    "                random.shuffle(batch_indices)\n",
    "\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating Dependent and Independent Variables\n",
    "\n",
    "The dataset is divided into independent variables (inputs/features) and dependent variables (outputs/targets) for each set (train, validation, and test). This separation is essential for training and evaluating machine learning models. The variables used are:\n",
    "\n",
    "- **Independent variables (features):**\n",
    "    - `arch_features` for training (includes KAN features and additional physics-informed features)\n",
    "    - `kan_features` for validation and test (only KAN input features)\n",
    "\n",
    "- **Dependent variables (targets):**\n",
    "    - `target` (the physical quantities to be predicted: x, z, px, pz)\n",
    "\n",
    "The tensors `X_train`, `X_val`, `X_test`, `y_train`, `y_val`, and `y_test` are created from the corresponding DataFrames for use in PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1730982595583,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "Xnz2g70QKxlX"
   },
   "outputs": [],
   "source": [
    "# Converting data to tensors\n",
    "X_train = torch.as_tensor(df_train_freq[arch_features].values, dtype=torch.float32)\n",
    "y_train = torch.as_tensor(df_train_freq[target].values, dtype=torch.float32)\n",
    "\n",
    "X_val = torch.as_tensor(df_val[kan_features].values, dtype=torch.float32)\n",
    "y_val = torch.as_tensor(df_val[target].values, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.as_tensor(df_test[kan_features].values, dtype=torch.float32)\n",
    "y_test = torch.as_tensor(df_test[target].values, dtype=torch.float32)\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Creating a loader for each dataset\n",
    "sampler = BalancedBatchSampler(train_dataset, batch_size=256)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_sampler=sampler,\n",
    "                        #   shuffle=False,\n",
    "                        #   batch_size=256,\n",
    "                          )\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        shuffle=False,\n",
    "                        batch_size=256\n",
    "                        )\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        shuffle=False,\n",
    "                        batch_size=len(test_dataset)\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `physics_loss_fn` is the core of the Physics-Informed Neural Network (PINN) approach, responsible for ensuring that the model's predictions adhere to the underlying physical principles of ray tracing. This function computes a composite loss that combines two critical components:\n",
    "\n",
    "1.  **Initial Condition Loss**: It first verifies that the model correctly reproduces the initial state of the system. For any data points where time `t=0`, it calculates the Mean Squared Error (MSE) between the model's predicted position `(x, z)` and the known initial position `(x0, z0)`. This forces the learned trajectories to start at the correct locations.\n",
    "\n",
    "2.  **Physics Residual Loss**: It then enforces the governing differential equations of ray tracing. Using automatic differentiation (`torch.autograd.grad`), it computes the derivatives of the model's outputs (x, z, px, pz) with respect to time `t`. These neural network-derived gradients are compared against the true derivatives provided by a classical numerical solver. The discrepancy between them forms the \"physics residual.\" This residual is weighted to account for non-uniform data density, ensuring that the model learns the physics consistently across the entire domain.\n",
    "\n",
    "The final loss is the sum of the initial condition loss and the weighted physics residual loss. By minimizing this composite loss, the model learns to generate trajectories that are not only consistent with the training data but also physically plausible according to the ray tracing equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730982595583,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "-d5i-smmDQ54"
   },
   "outputs": [],
   "source": [
    "# Adjusted physical loss function\n",
    "def physics_loss_fn(model, input_tensor, *args, **kwargs):\n",
    "    # Separate the inputs and ensure that the variables require gradient\n",
    "    x0 = input_tensor[:, 0].view(-1, 1)  # x0\n",
    "    z0 = input_tensor[:, 1].view(-1, 1)  # z0\n",
    "    theta0 = input_tensor[:, 2].view(-1, 1)  # theta0\n",
    "    t_var = input_tensor[:, 3].view(-1, 1)  # t\n",
    "    \n",
    "    # Weights for each sample\n",
    "    weights = input_tensor[:, 4].view(-1, 1)\n",
    "    \n",
    "    # Getting the derivatives from the non-parametric model\n",
    "    dxdt_true = input_tensor[:, 5].view(-1, 1)\n",
    "    dzdt_true = input_tensor[:, 6].view(-1, 1)\n",
    "    dpxdt_true = input_tensor[:, 7].view(-1, 1)\n",
    "    dpzdt_true = input_tensor[:, 8].view(-1, 1)\n",
    "\n",
    "    # Make sure all variables have requires_grad=True to calculate derivatives\n",
    "    t_var.requires_grad_(True)\n",
    "\n",
    "    # Recalculate yhat using the variables to ensure the connection\n",
    "    x_reconstructed = torch.cat([x0, z0, theta0, t_var], dim=1)\n",
    "    yhat = model(x_reconstructed)\n",
    "    \n",
    "    x_out = yhat[:, 0].view(-1, 1)\n",
    "    z_out = yhat[:, 1].view(-1, 1)\n",
    "    px_out = yhat[:, 2].view(-1, 1)\n",
    "    pz_out = yhat[:, 3].view(-1, 1)\n",
    "    \n",
    "    # If t exists in the input tensor\n",
    "    t_zero_mask = (t_var == 0).squeeze()\n",
    "    if t_zero_mask.any():\n",
    "        x0_t0 = x0[t_zero_mask]\n",
    "        z0_t0 = z0[t_zero_mask]\n",
    "        \n",
    "        # Ensure that in t = 0, x = x0 and z = z0\n",
    "        x_initial_condition = (yhat[t_zero_mask, 0] - x0_t0) ** 2\n",
    "        z_initial_condition = (yhat[t_zero_mask, 1] - z0_t0) ** 2\n",
    "        initial_condition_loss = torch.mean(x_initial_condition + z_initial_condition)\n",
    "    else:\n",
    "        # If t=0 is not present, the loss is 0\n",
    "        initial_condition_loss = 0\n",
    "    \n",
    "\n",
    "    # Calculate the necessary gradients\n",
    "    dxdt = torch.autograd.grad(\n",
    "        outputs=x_out,  # Derivative of x with respect to t\n",
    "        inputs=t_var,\n",
    "        grad_outputs=torch.ones_like(x_out),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    dzdt = torch.autograd.grad(\n",
    "        outputs=z_out,  # Derivative of z with respect to t\n",
    "        inputs=t_var,\n",
    "        grad_outputs=torch.ones_like(z_out),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    dpxdt = torch.autograd.grad(\n",
    "        outputs=px_out,  # Derivative of px with respect to t\n",
    "        inputs=t_var,\n",
    "        grad_outputs=torch.ones_like(px_out),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    dpzdt = torch.autograd.grad(\n",
    "        outputs=pz_out,  # Derivative of pz with respect to t\n",
    "        inputs=t_var,\n",
    "        grad_outputs=torch.ones_like(pz_out),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Differential equations according to your figure\n",
    "    pde1 = dxdt - dxdt_true\n",
    "    pde2 = dzdt - dzdt_true\n",
    "    pde3 = dpxdt - dpxdt_true\n",
    "    pde4 = dpzdt - dpzdt_true\n",
    "\n",
    "    # Calculate the loss as the sum of the squared errors of the PDEs\n",
    "    residual = pde1**2 + pde2**2 + pde3**2 + pde4**2\n",
    "    loss = torch.sum(weights * residual) / torch.sum(weights)\n",
    "    return loss + initial_condition_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the optimal hyperparameters determined, we can now construct and train the definitive Physics-Informed Kolmogorov-Arnold Network (PIKAN) model.\n",
    "\n",
    "This involves several key steps:\n",
    "1.  **Model Definition**: We define the KAN architecture with a specific structure: `width=[4, 12, 6, 4]`, a grid size of `12`, and a spline order `k=3`. This configuration was identified as providing a good balance between expressiveness and complexity for this problem.\n",
    "2.  **Optimizer and Scheduler**: We use the `Adam` optimizer with a learning rate of `1e-2`. To facilitate stable convergence, a `ReduceLROnPlateau` learning rate scheduler is also employed, which will decrease the learning rate if the validation loss plateaus.\n",
    "3.  **Loss Function**: The training will be guided by a composite loss strategy. We use a standard `MSELoss` for the data-fidelity term and our custom `physics_loss_fn` to enforce the physical constraints of ray tracing.\n",
    "4.  **Architecture Instantiation**: All these components—the model, optimizer, scheduler, and loss functions—are encapsulated within our `Architecture` helper class. This class manages the training loop, applies the physics loss with its specific weight (`lambda_physics=1e-3`), and handles early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "input_size = len(kan_features)\n",
    "output_size = len(target)\n",
    "\n",
    "model = KAN(width=[input_size, 12, 6, output_size],\n",
    "            grid=12,\n",
    "            grid_range=[-5, 5],\n",
    "            k=3,\n",
    "            auto_save=False,\n",
    "            ckpt_path=str(CHECKPOINT_PATH),\n",
    "            seed=SEED,\n",
    "            device=DEVICE)\n",
    "\n",
    "# Defines optimizer\n",
    "optimizer = partial(optim.Adam, lr=1e-2)\n",
    "scheduler = partial(optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    mode='min',\n",
    "                    factor=0.1,\n",
    "                    patience=5,\n",
    "                    min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = Architecture(model=model,\n",
    "                    loss_fn=loss_fn,\n",
    "                    physics_fn=physics_loss_fn,\n",
    "                    partial_optimizer=optimizer,\n",
    "                    partial_scheduler=scheduler,\n",
    "                    use_weighted_pi=True, \n",
    "                    lamb=0,\n",
    "                    lamb_l1=0,\n",
    "                    lamb_entropy=0,\n",
    "                    lamb_coef=0,\n",
    "                    lamb_coefdiff=0,\n",
    "                    lambda_physics=1e-3,\n",
    "                    singularity_avoiding=True,\n",
    "                    device=DEVICE)\n",
    "\n",
    "arch.set_loaders(train_loader, val_loader)\n",
    "arch.set_early_stopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1730982837406,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "kCMiGYGwvTz-"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "arch.train(n_epochs=n_epochs, seed=SEED)\n",
    "arch.save_checkpoint(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 1343,
     "status": "ok",
     "timestamp": 1730928212869,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "KsuYlbBgP8ad",
    "outputId": "6ff6f482-9430-4094-a0a9-6499e15857ab"
   },
   "outputs": [],
   "source": [
    "fig = arch.plot_losses()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After training the model, the next crucial step is to evaluate its performance on unseen data. This process allows us to assess how well the model has generalized from the training set to new, independent examples.\n",
    "\n",
    "Our evaluation pipeline involves the following steps:\n",
    "1.  **Load the Trained Model**: We first load the best-performing model checkpoint that was saved during training.\n",
    "2.  **Prepare the Test Dataset**: We use the `test_loader`, which contains data that the model has never seen before.\n",
    "3.  **Make Predictions**: The trained model is used to predict the ray trajectories for the inputs in the test set.\n",
    "4.  **Quantitative Analysis**: We calculate quantitative metrics to measure the discrepancy between the model's predictions and the ground truth from the numerical solver.\n",
    "5.  **Qualitative Analysis**: We generate plots to visually compare the predicted ray paths against the true paths, overlaid on the Marmousi velocity model. This provides an intuitive understanding of the model's accuracy and where errors might be occurring.\n",
    "\n",
    "This comprehensive evaluation ensures that we have a robust understanding of the model's capabilities and limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "input_size = len(kan_features)\n",
    "output_size = len(target)\n",
    "\n",
    "trained_model = KAN(width=[input_size, 12, 6, output_size],\n",
    "                    grid=12,\n",
    "                    grid_range=[-5, 5],\n",
    "                    k=3,\n",
    "                    auto_save=False,\n",
    "                    ckpt_path=str(CHECKPOINT_PATH),\n",
    "                    seed=SEED,\n",
    "                    device=DEVICE)\n",
    "\n",
    "# Defines optimizer\n",
    "optimizer = partial(optim.Adam, lr=1e-2)\n",
    "scheduler = partial(optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    mode='min',\n",
    "                    factor=0.1,\n",
    "                    patience=5,\n",
    "                    min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "trained_arch = Architecture(model=trained_model,\n",
    "                            loss_fn=loss_fn,\n",
    "                            physics_fn=None,\n",
    "                            partial_optimizer=optimizer,\n",
    "                            partial_scheduler=scheduler,\n",
    "                            use_weighted_pi=True,\n",
    "                            lamb=0,\n",
    "                            lamb_l1=0,\n",
    "                            lamb_entropy=0,\n",
    "                            lamb_coef=0,\n",
    "                            lamb_coefdiff=0,\n",
    "                            lambda_physics=1e-3,\n",
    "                            singularity_avoiding=True,\n",
    "                            device=DEVICE\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_arch.load_checkpoint(model_save_path)\n",
    "df_test = pd.read_csv(DATA_PATH/'test_marmousi_f30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1730982850010,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "hLuSEQMxJCGT",
    "outputId": "4d310f19-e960-487c-9dc0-5b2821432849"
   },
   "outputs": [],
   "source": [
    "predictions = trained_arch.predict(df_test[kan_features].values)\n",
    "df_pred = pd.DataFrame(predictions, columns=target)\n",
    "df_pred = df_test[kan_features].join(df_pred)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(df_pred[target].values, df_test[target].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "executionInfo": {
     "elapsed": 5252,
     "status": "ok",
     "timestamp": 1730982895623,
     "user": {
      "displayName": "Lucas Torres Marques",
      "userId": "18173835100429389614"
     },
     "user_tz": 180
    },
    "id": "VymCRB4dXrAU",
    "outputId": "93478670-6e6f-4a72-bd71-10d15d97a22a"
   },
   "outputs": [],
   "source": [
    "# Number of desired subplots\n",
    "unique_initial_conditions = (df_test[kan_features]\n",
    "                             .drop(columns=['t'])\n",
    "                             .drop_duplicates()\n",
    "                             .sample(9, random_state=SEED)\n",
    "                             )\n",
    "n_initial_conditions = unique_initial_conditions.shape[0]\n",
    "cols = 3\n",
    "rows = (n_initial_conditions + cols - 1) // cols\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(22, 12))\n",
    "axs = axs.flatten()  # Flatten to iterate easily\n",
    "\n",
    "# Find the minimum and maximum values of data_gen.Vbsplines to set the color bar dynamically\n",
    "vmin, vmax = np.min(data_gen.Vbsplines), np.max(data_gen.Vbsplines)\n",
    "\n",
    "for ax, (x0, z0, theta0_p) in zip(axs[:n_initial_conditions], unique_initial_conditions.values):\n",
    "    initial_condition = (\n",
    "        (df_test['x0'] == x0) &\n",
    "        (df_test['z0'] == z0) &\n",
    "        (df_test['theta0_p'] == theta0_p)\n",
    "    )\n",
    "\n",
    "    im = ax.imshow(np.flipud(data_gen.Vbsplines), extent=[data_gen.x_range[0],\n",
    "                                                          data_gen.x_range[1],\n",
    "                                                          data_gen.z_range[0],\n",
    "                                                          data_gen.z_range[-1]],\n",
    "                   vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f'{x0=:.2f}, {z0=:.2f}, {theta0_p=:.2f}')\n",
    "    ax.plot(df_test.loc[initial_condition, 'x'],\n",
    "            df_test.loc[initial_condition, 'z'],\n",
    "            color='grey',\n",
    "            linewidth=2,\n",
    "            label='True Path'\n",
    "            )\n",
    "    ax.plot(df_pred.loc[initial_condition, 'x'],\n",
    "            df_pred.loc[initial_condition, 'z'],\n",
    "            color='black',\n",
    "            linewidth=2,\n",
    "            label='Predicted Path'\n",
    "            )\n",
    "    for zi in data_gen.z:\n",
    "        ax.plot(data_gen.x, zi *\n",
    "                np.ones_like(data_gen.x), 'kx', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('x (km)')\n",
    "    ax.set_ylabel('z (km)')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    ax.set_xlim([3.5, 6.5])\n",
    "    ax.set_ylim([2.5, 0])\n",
    "\n",
    "# Add a general color bar for the entire figure\n",
    "# Set the position of the color bar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "# Add a single legend outside the subplot area\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='grey', lw=2, label='True Path'),\n",
    "    Line2D([0], [0], color='black', lw=2, label='Predicted Path'),\n",
    "    Line2D([0], [0], color='k', marker='x', lw=0, label='Z markers')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='center right', bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "fig.suptitle('Visualization of Velocity Models and Predicted Paths', fontsize=16, y=1.02)\n",
    "\n",
    "for ax in axs[n_initial_conditions:]:\n",
    "    ax.axis('off')  # Disable axes for empty subplots\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.subplots_adjust(\n",
    "    left=0.05,\n",
    "    top=0.92,\n",
    "    bottom=0.08,\n",
    "    wspace=0.0,  # horizontal space between subplots\n",
    "    hspace=0.30   # vertical space between subplots\n",
    ")  # Adjust space for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the background map: Vbsplines\n",
    "im = ax.imshow(np.flipud(data_gen.Vbsplines), \n",
    "               extent=[data_gen.x_range[0], data_gen.x_range[1],\n",
    "                       data_gen.z_range[0], data_gen.z_range[-1]],\n",
    "               vmin=np.min(data_gen.Vbsplines),\n",
    "               vmax=np.max(data_gen.Vbsplines),\n",
    "               aspect='auto')\n",
    "\n",
    "unique_initial_conditions = (df_test[kan_features]\n",
    "                             .drop(columns=['t'])\n",
    "                             .drop_duplicates()\n",
    "                             .sample(6, random_state=SEED)\n",
    "                             )\n",
    "\n",
    "# Plot all real trajectories\n",
    "\n",
    "for x0, z0, theta0_p in unique_initial_conditions.values:\n",
    "    plot_test = (\n",
    "        (df_test['x0'] == x0) &\n",
    "        (df_test['z0'] == z0) &\n",
    "        (df_test['theta0_p'] == theta0_p)\n",
    "    )\n",
    "    plot_pred = (\n",
    "        (df_pred['x0'] == x0) &\n",
    "        (df_pred['z0'] == z0) &\n",
    "        (df_pred['theta0_p'] == theta0_p)\n",
    "    )\n",
    "    ax.plot(df_test[plot_test]['x'],\n",
    "            df_test[plot_test]['z'],\n",
    "            color='grey',\n",
    "            marker='x',\n",
    "            markevery=100,\n",
    "            linestyle='--',\n",
    "            linewidth=1.5\n",
    "            )\n",
    "    ax.plot(df_pred[plot_pred]['x'],\n",
    "            df_pred[plot_pred]['z'],\n",
    "            color='black',\n",
    "            linewidth=1.5)\n",
    "\n",
    "# Mark the Z points\n",
    "for zi in data_gen.z:\n",
    "    ax.plot(data_gen.x, zi * np.ones_like(data_gen.x), 'kx', linewidth=1)\n",
    "\n",
    "# Graph adjustments\n",
    "ax.set_xlabel('x (km)')\n",
    "ax.set_ylabel('z (km)')\n",
    "# ax.set_title('Runge-Kutta vs PIKAN predicted paths')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlim([3.5, 6.5])\n",
    "ax.set_ylim([2.5, 0])\n",
    "\n",
    "# Add color bar\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label('Velocity (km/s)')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='grey', lw=2, label='True Path'),\n",
    "    Line2D([0], [0], color='black', lw=2, label='Predicted Path'),\n",
    "    Line2D([0], [0], color='k', marker='x', lw=0, label='Z markers')\n",
    "]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to calculate the mean absolute percentage error for each ray trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[initial_condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_initial_conditions = (df_test[kan_features]\n",
    "                             .drop(columns=['t', 'theta0_p'])\n",
    "                             .drop_duplicates())\n",
    "\n",
    "df_error = pd.DataFrame()\n",
    "\n",
    "for _, row in unique_initial_conditions.iterrows():\n",
    "    x0, z0 = row\n",
    "    initial_condition = (\n",
    "        (df_pred['x0'] == x0) &\n",
    "        (df_pred['z0'] == z0)\n",
    "    )\n",
    "    score_value = score(df_pred[initial_condition][target].values,\n",
    "                        df_test[initial_condition][target].values)\n",
    "    mse = score_value.iloc[2, :]\n",
    "    aux = row.copy()\n",
    "    aux['x.mse'] = mse.iloc[0]\n",
    "    aux['z.mse'] = mse.iloc[1]\n",
    "    aux['px.mse'] = mse.iloc[2]\n",
    "    aux['pz.mse'] = mse.iloc[3]\n",
    "    df_error = pd.concat([df_error, aux.to_frame().T], ignore_index=True)\n",
    "\n",
    "df_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_differences = (\n",
    "    df_error.groupby(['x0', 'z0'], as_index=False)[['x.mse', 'z.mse']]\n",
    "    .mean()\n",
    ")\n",
    "mean_differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(df: pd.DataFrame, col: str) -> None:\n",
    "    # Organizing data for the 2D surface plot matrix\n",
    "    x = np.sort(df['x0'].unique())  # Unique sorted x0 values\n",
    "    z = np.sort(df['z0'].unique())  # Unique sorted z0 values\n",
    "\n",
    "    # Create a matrix for the mean error values\n",
    "    z_matrix = np.zeros((len(x), len(z)))\n",
    "\n",
    "    # Fill the matrix with mean error values\n",
    "    for i, x_val in enumerate(x):  # Iterate over x0 values\n",
    "        for j, z_val in enumerate(z):  # Iterate over z0 values\n",
    "            filter_cond = (df['x0'] == x_val) & (\n",
    "                df['z0'] == z_val)\n",
    "            if filter_cond.any():  # If at least one match\n",
    "                z_matrix[i, j] = df.loc[filter_cond, col].values[0]\n",
    "\n",
    "    # Create the 2D contour plot (Surface plot in 2D)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=np.flipud(data_gen.Vbsplines),\n",
    "            x=np.linspace(0, xmax, vp.shape[1]),   # Assign the x axis range\n",
    "            y=np.linspace(0, zmax, vp.shape[0]),   # Assign the z axis range\n",
    "            colorscale='Viridis',  # Set the color scale\n",
    "            showscale=False\n",
    "        ))\n",
    "\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            z=z_matrix,\n",
    "            x=x,\n",
    "            y=z,\n",
    "            colorscale='Inferno',\n",
    "            contours=dict(\n",
    "                showlines=True,  # Show contour lines\n",
    "                coloring='lines',  # Color only the lines\n",
    "            ),\n",
    "            line=dict(\n",
    "                width=5,  # Thicker contour lines\n",
    "            ),\n",
    "            colorbar=dict(\n",
    "            tickformat=\".2e\",                # ← scientific notation\n",
    "            tickfont=dict(size=16),\n",
    "            exponentformat=\"e\"               # 1e+03 style (optional)\n",
    "        )\n",
    "        ))\n",
    "\n",
    "    # Customize the layout of the plot\n",
    "    fig.update_layout(\n",
    "        xaxis_title='x0 (km)',\n",
    "        yaxis_title='z0 (km)',\n",
    "        width=800, height=700,\n",
    "        margin=dict(l=20, r=20, b=20, t=30),  # Tight margin to reduce unused space\n",
    "        yaxis=dict(\n",
    "            range=[3, 1],  # Reverse the Y-axis (z0 values)\n",
    "            tickfont=dict(size=16),\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            range=[4, 5],  # Reverse the X-axis (x0 values)\n",
    "            tickfont=dict(size=16),\n",
    "        ),\n",
    "        xaxis_title_font=dict(size=20),  # Increase X-axis title font size\n",
    "        yaxis_title_font=dict(size=20),  # Increase Y-axis title font size\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(mean_differences, 'x.mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(mean_differences, 'z.mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction time comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's evaluate the Runge-Kutta method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DataGeneratorMarmousi(\n",
    "    x_range=x_range,\n",
    "    z_range=z_range\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_initial_conditions = (df_test[kan_features]\n",
    "                             .drop(columns=['t'])\n",
    "                             .drop_duplicates())\n",
    "unique_initial_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are tracing 323 ray trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 7\n",
    "\n",
    "rays = data_gen.run_batch(x0_vec=unique_initial_conditions['x0'],\n",
    "                          z0_vec=unique_initial_conditions['z0'],\n",
    "                          theta0_vec=unique_initial_conditions['theta0_p'],\n",
    "                          vp=vp,\n",
    "                          factor=30,\n",
    "                          t_max=0.4\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Second order Runge-Kutta method took `29 s ± 99.4 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)`. But 13 seconds are basically the B-splines filter, so we can consider a 16 s ± 99.4 ms to be fair.\n",
    "\n",
    "Now, we can compare with the proposed PIKAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 7\n",
    "\n",
    "predictions = arch.predict(df_test[kan_features].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the proposed PIKAN model executed in just `399 ms ± 10.3 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)`, making it approximately 40× faster than the baseline implementation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tKWQ0AuHya7A"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "exai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
